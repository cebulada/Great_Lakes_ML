{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to save MinMax Scaler to apply to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"lake\"].copy()\n",
    "y.to_frame()\n",
    "y.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = y.replace([\"erie\", \"huron\", \"ontario\", \"superior\"], [0, 1, 2, 3]).copy()\n",
    "y_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_cat.values.reshape(-1, 1)\n",
    "y_num = y_cat.to_numpy(copy=True)\n",
    "type(y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reshape = y_num.reshape(-1, 1)\n",
    "y = y_reshape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data array\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "X_array = X.values.copy()\n",
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "X_finite = imp.transform(X_array)\n",
    "X_finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more missing data is present\n",
    "np.isnan(X_finite).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# the scaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the data, compute the statistics\n",
    "scaler.fit(X_finite)\n",
    "\n",
    "# save the scaler for later application\n",
    "from joblib import dump, load\n",
    "scaler_minmax = os.path.join(home, \"models\", \"website\", 'min_max_scaler.scaler')\n",
    "dump(scaler, scaler_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "scaler = load(scaler_minmax)\n",
    "X_train_scaled = scaler.transform(X_finite)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"lake\"].copy()\n",
    "y.to_frame()\n",
    "y.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = y.replace([\"erie\", \"huron\", \"ontario\", \"superior\"], [0, 1, 2, 3]).copy()\n",
    "y_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_cat.values.reshape(-1, 1)\n",
    "y_num = y_cat.to_numpy(copy=True)\n",
    "type(y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reshape = y_num.reshape(-1, 1)\n",
    "y = y_reshape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data array\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "X_array = X.values.copy()\n",
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "imputer_simple = os.path.join(home, \"models\", \"website\", 'SimpleImputer.imputer')\n",
    "dump(imp, imputer_simple)\n",
    "\n",
    "# load it back\n",
    "imp = load(imputer_simple)\n",
    "\n",
    "X_finite = imp.transform(X_array)\n",
    "X_finite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try Linear Regression First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"lake\"].copy()\n",
    "y.to_frame()\n",
    "y.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = y.replace([\"erie\", \"huron\", \"ontario\", \"superior\"], [0, 1, 2, 3]).copy()\n",
    "y_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_cat.values.reshape(-1, 1)\n",
    "y_num = y_cat.to_numpy(copy=True)\n",
    "type(y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reshape = y_num.reshape(-1, 1)\n",
    "y = y_reshape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data array\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "X_array = X.values.copy()\n",
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "# stratify because the labels are not in equal proportions\n",
    "# random_state=42 for all models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)\n",
    "X_train_finite = imp.transform(X_train)\n",
    "X_test_finite = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more missing data is present\n",
    "np.isnan(X_train_finite).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# the scaler instance\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# fit the data, compute the statistics\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# use statistics that have been learned to transform the training and testing data\n",
    "X_train_scaled = scaler.transform(X_train_finite)\n",
    "X_test_scaled = scaler.transform(X_test_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a line in 33D space using least squares method\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit Linearregression on training data\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope of the line and intercept of the line\n",
    "print('Weight coefficients: \\n', regressor.coef_)\n",
    "print('y-axis intercept: \\n', regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on training data\n",
    "print(f\"Score on Training Data: {regressor.score(X_train_scaled, y_train)}\")\n",
    "      \n",
    "# Score on testing data\n",
    "print(f\"Score on Testing Data: {regressor.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=.25)\n",
    "\n",
    "score_train = cross_val_score(regressor, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "score_train_shuffle = cross_val_score(regressor, X_train_scaled, y_train, cv=cv)\n",
    "\n",
    "score_test = cross_val_score(regressor, X_test_scaled, y_test, cv=5)\n",
    "\n",
    "score_test_shuffle = cross_val_score(regressor, X_test_scaled, y_test, cv=cv)\n",
    "\n",
    "print('Scores on each train CV fold: %s' % score_train)\n",
    "print('Mean score: %0.3f' % np.mean(score_train))\n",
    "\n",
    "print('Scores on each train shuffle CV fold: %s' % score_train_shuffle)\n",
    "print('Mean score: %0.3f' % np.mean(score_train_shuffle))\n",
    "\n",
    "print('Scores on each test CV fold: %s' % score_test)\n",
    "print('Mean score: %0.3f' % np.mean(score_test))\n",
    "\n",
    "print('Scores on each test shuffle CV fold: %s' % score_test_shuffle)\n",
    "print('Mean score: %0.3f' % np.mean(score_test_shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "predictions = regressor.predict(X_test_scaled)\n",
    "# Plot Residuals\n",
    "plt.scatter(predictions, predictions - y_test)\n",
    "plt.hlines(y=0, xmin=predictions.min(), xmax=predictions.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression next\n",
    "# Due to one hot encoding cannot do logistic regression\n",
    "# Try K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_array)\n",
    "\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn panda dataseries into numpy arrays\n",
    "X_array = X.values.copy()\n",
    "\n",
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(encode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "# stratify because the labels are not in equal proportions\n",
    "# random_state=42 for all models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, encode_y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=encode_y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)\n",
    "X_train_finite = imp.transform(X_train)\n",
    "X_test_finite = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify=y_array preserves the original class proportions in percent\n",
    "print('All:', np.bincount(encode_y) / float(len(encode_y)) * 100.0)\n",
    "print('Training:', np.bincount(y_train) / float(len(y_train)) * 100.0)\n",
    "print('Test:', np.bincount(y_test) / float(len(y_test)) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaler.fit(X_train_finite)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_finite)\n",
    "X_test_scaled = scaler.transform(X_test_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "diff = []\n",
    "for k in range(1, 30, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-2)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    diff.append(train_score-test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}/{(train_score-test_score):.3f}\")\n",
    "    \n",
    "print(f\"Min Difference: {(min(diff)):.3f}\")\n",
    "\n",
    "plt.plot(range(1, 30, 2), train_scores, marker='o', label=\"Training Accuracy\")\n",
    "plt.plot(range(1, 30, 2), test_scores, marker=\"x\", label=\"Testing Accuracy\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.title(\"K Neighbours Classifier\")\n",
    "plt.legend(loc='best')\n",
    "plt.yticks(np.arange(0.83, 0.98, step=0.01))\n",
    "\n",
    "fig_knn = os.path.join(home, \"models\", 'knn.png')\n",
    "plt.savefig(fig_knn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# use k=9 get 86% testing accuracy\n",
    "knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-2)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model with 100% of data for unknown determinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)\n",
    "\n",
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_array)\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "X_array = X.values.copy()\n",
    "\n",
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "X_array_finite = imp.transform(X_array)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_array_finite)\n",
    "X_array_scaled = scaler.transform(X_array_finite)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# use k=9 get 86% testing accuracy\n",
    "knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-2)\n",
    "knn.fit(X_array_scaled, encode_y)\n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "model_website_knn = os.path.join(home, \"models\", \"website\", 'knn_k-9.joblib')\n",
    "dump(knn, model_website_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_array)\n",
    "\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn panda dataseries into numpy arrays\n",
    "X_array = X.values.copy()\n",
    "\n",
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(encode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "# stratify because the labels are not in equal proportions\n",
    "# random_state=42 for all models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, encode_y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=encode_y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)\n",
    "X_train_finite = imp.transform(X_train)\n",
    "X_test_finite = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no more missing data is present\n",
    "np.isnan(X_train_finite).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify=y_array preserves the original class proportions in percent\n",
    "print('All:', np.bincount(encode_y) / float(len(encode_y)) * 100.0)\n",
    "print('Training:', np.bincount(y_train) / float(len(y_train)) * 100.0)\n",
    "print('Test:', np.bincount(y_test) / float(len(y_test)) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaler.fit(X_train_finite)\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_finite)\n",
    "X_test_scaled = scaler.transform(X_test_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "diff = []\n",
    "range_estimators = [5, 10, 25, 50, 75, 100, 250, 500]\n",
    "\n",
    "for est in range_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators=est, n_jobs=-2)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    train_score = rf.score(X_train_scaled, y_train)\n",
    "    test_score = rf.score(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    diff.append(train_score-test_score)\n",
    "    print(f\"estimators: {est}, Train/Test Score: {train_score:.3f}/{test_score:.3f}/{(train_score-test_score):.3f}\")\n",
    "    \n",
    "print(f\"Min Difference: {(min(diff)):.3f}\")\n",
    "\n",
    "plt.plot(range_estimators, train_scores, marker='o', label=\"Training Accuracy\")\n",
    "plt.plot(range_estimators, test_scores, marker=\"x\", label=\"Testing Accuracy\")\n",
    "plt.xlabel(\"Random Forest Classifier - Number of Estimators\")\n",
    "plt.ylabel(\"Testing Accuracy Score\")\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Random Forest Classifier\")\n",
    "plt.yticks(np.arange(0.89, 0.97, step=0.005))\n",
    "\n",
    "fig_rf = os.path.join(home, \"models\", 'rf.png')\n",
    "plt.savefig(fig_rf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# use n_estimators=100 get 90% testing accuracy and 46 MB model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-2)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "print(rf.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence, use k=7\n",
    "from joblib import dump, load\n",
    "\n",
    "model_rf = os.path.join(home, \"models\", 'rf_est-100.joblib')\n",
    "\n",
    "dump(rf, model_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = load(model_rf)\n",
    "rf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model with 100% of data for unknown determinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)\n",
    "\n",
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_array)\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "X_array = X.values.copy()\n",
    "\n",
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "X_array_finite = imp.transform(X_array)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_array_finite)\n",
    "X_array_scaled = scaler.transform(X_array_finite)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# use n_estimators=100 get 90% testing accuracy\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-2)\n",
    "rf.fit(X_array_scaled, encode_y)\n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "model_website_rf = os.path.join(home, \"models\", \"website\", 'rf_est-100.joblib')\n",
    "dump(rf, model_website_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try some deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "X_array = X.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_array)\n",
    "\n",
    "encode_y = label_encoder.transform(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 categories do not have equal distributions\n",
    "# will need to stratify the categories\n",
    "# make sure the training and testing data have equal distributions of each category\n",
    "np.bincount(encode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "# stratify because the labels are not in equal proportions\n",
    "# random_state=42 for all models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, encode_y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=encode_y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)\n",
    "X_train_finite = imp.transform(X_train)\n",
    "X_test_finite = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify=y_array preserves the original class proportions in percent\n",
    "print('All:', np.bincount(encode_y) / float(len(encode_y)) * 100.0)\n",
    "print('Training:', np.bincount(y_train) / float(len(y_train)) * 100.0)\n",
    "print('Test:', np.bincount(y_test) / float(len(y_test)) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train_finite)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_finite)\n",
    "X_test_scaled = scaler.transform(X_test_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "train_scores = []\n",
    "train_losses = []\n",
    "test_scores = []\n",
    "test_losses = []\n",
    "\n",
    "diff = []\n",
    "\n",
    "number_hidden_nodes = 18\n",
    "number_inputs = 25\n",
    "number_classes = 4\n",
    "\n",
    "for b in range(10):\n",
    "    \n",
    "    deep_model = Sequential()\n",
    "\n",
    "    deep_model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=number_inputs))\n",
    "\n",
    "    for a in range(b):\n",
    "        deep_model.add(Dense(units=number_hidden_nodes, activation='relu'))\n",
    "\n",
    "    deep_model.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "    deep_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    deep_model.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_categorical,\n",
    "            epochs=200,\n",
    "            shuffle=True,\n",
    "            verbose=2,\n",
    "            workers=3\n",
    "            )\n",
    "    \n",
    "    model_loss_train, model_accuracy_train = deep_model.evaluate(\n",
    "            X_train_scaled, y_train_categorical, verbose=2)\n",
    "    model_loss_test, model_accuracy_test = deep_model.evaluate(\n",
    "            X_test_scaled, y_test_categorical, verbose=2)\n",
    "    \n",
    "    train_scores.append(model_accuracy_train)\n",
    "    train_losses.append(model_loss_train)\n",
    "    test_scores.append(model_accuracy_test)\n",
    "    test_losses.append(model_loss_test)\n",
    "    diff.append(model_accuracy_train-model_accuracy_test)\n",
    "    \n",
    "    print(f\"Hidden Layers: {b+1}, Train/Test Score: {model_accuracy_train:.3f}/{model_accuracy_test:.3f}/{(model_accuracy_train-model_accuracy_test):.3f}\")\n",
    "    \n",
    "    del deep_model\n",
    "    \n",
    "print(f\"Min Difference: {(min(diff)):.3f}\")\n",
    "\n",
    "plt.plot(range(1, 11, 1), train_scores, marker='o', label=\"Training Accuracy\")\n",
    "plt.plot(range(1, 11, 1), test_scores, marker=\"x\", label=\"Testing Accuracy\")\n",
    "plt.plot(range(1, 11, 1), train_losses, marker=\"<\", label=\"Training Loss\")\n",
    "plt.plot(range(1, 11, 1), test_losses, marker=\">\", label=\"Testing Loss\")\n",
    "\n",
    "plt.xlabel(\"Number of Hidden Layers - 18 hidden nodes\")\n",
    "plt.ylabel(\"Testing Accuracy / Losses Score\")\n",
    "plt.title(\"Deep Neural Net\")\n",
    "plt.legend(loc='best')\n",
    "plt.yticks(np.arange(0.3, 1, step=0.1))\n",
    "plt.xticks(np.arange(0, 11, step=1))\n",
    "\n",
    "fig_deep = os.path.join(home, \"models\", 'deep_neural_num_hidden.png')\n",
    "plt.savefig(fig_deep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "train_scores = []\n",
    "train_losses = []\n",
    "test_scores = []\n",
    "test_losses = []\n",
    "\n",
    "diff = []\n",
    "\n",
    "number_hidden_nodes = 18\n",
    "number_inputs = 25\n",
    "number_classes = 4\n",
    "\n",
    "number_epochs = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
    "\n",
    "for b in number_epochs:\n",
    "    \n",
    "    deep_model = Sequential()\n",
    "\n",
    "    deep_model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=number_inputs))\n",
    "\n",
    "    for a in range(7):\n",
    "        deep_model.add(Dense(units=number_hidden_nodes, activation='relu'))\n",
    "\n",
    "    deep_model.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "    deep_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    deep_model.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_categorical,\n",
    "            epochs=b,\n",
    "            shuffle=True,\n",
    "            verbose=2,\n",
    "            workers=3\n",
    "            )\n",
    "    \n",
    "    model_loss_train, model_accuracy_train = deep_model.evaluate(\n",
    "            X_train_scaled, y_train_categorical, verbose=2)\n",
    "    model_loss_test, model_accuracy_test = deep_model.evaluate(\n",
    "            X_test_scaled, y_test_categorical, verbose=2)\n",
    "    \n",
    "    train_scores.append(model_accuracy_train)\n",
    "    train_losses.append(model_loss_train)\n",
    "    test_scores.append(model_accuracy_test)\n",
    "    test_losses.append(model_loss_test)\n",
    "    diff.append(model_accuracy_train-model_accuracy_test)\n",
    "    \n",
    "    print(f\"Hidden Layers: {7}, Train/Test Score: {model_accuracy_train:.3f}/{model_accuracy_test:.3f}/{(model_accuracy_train-model_accuracy_test):.3f}\")\n",
    "    \n",
    "    del deep_model\n",
    "    \n",
    "print(f\"Min Difference: {(min(diff)):.3f}\")\n",
    "\n",
    "plt.plot(number_epochs, train_scores, marker='o', label=\"Training Accuracy\")\n",
    "plt.plot(number_epochs, test_scores, marker=\"x\", label=\"Testing Accuracy\")\n",
    "plt.plot(number_epochs, train_losses, marker=\"<\", label=\"Training Loss\")\n",
    "plt.plot(number_epochs, test_losses, marker=\">\", label=\"Testing Loss\")\n",
    "\n",
    "plt.xlabel(\"Number of Epochs - 7 Hidden Layers, 18 nodes each\")\n",
    "plt.ylabel(\"Testing Accuracy / Losses Score\")\n",
    "plt.title(\"Deep Neural Net\")\n",
    "plt.legend(loc='best')\n",
    "plt.yticks(np.arange(0.3, 1, step=0.1))\n",
    "\n",
    "fig_deep_epoch = os.path.join(home, \"models\", 'deep_neural_num_epoch.png')\n",
    "plt.savefig(fig_deep_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "number_hidden_nodes = 18\n",
    "number_inputs = 25\n",
    "number_classes = 4\n",
    "\n",
    "deep_model = Sequential()\n",
    "\n",
    "deep_model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=number_inputs))\n",
    "\n",
    "for a in range(7):\n",
    "    deep_model.add(Dense(units=number_hidden_nodes, activation='relu'))\n",
    "\n",
    "deep_model.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "deep_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_categorical,\n",
    "        epochs=1000,\n",
    "        shuffle=True,\n",
    "        verbose=2,\n",
    "        workers=3\n",
    "        )\n",
    "\n",
    "model_loss_train, model_accuracy_train = deep_model.evaluate(\n",
    "        X_train_scaled, y_train_categorical, verbose=2)\n",
    "model_loss_test, model_accuracy_test = deep_model.evaluate(\n",
    "        X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(model_accuracy_train)\n",
    "print(model_accuracy_test)\n",
    "\n",
    "# Save the model\n",
    "deep_path = os.path.join(home, \"models\", 'deep_neural_7_hidden_1000_epoch.h5')\n",
    "\n",
    "deep_model.save(deep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "deep_model = load_model(deep_path)\n",
    "\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss_train, model_accuracy_train = deep_model.evaluate(\n",
    "        X_train_scaled, y_train_categorical, verbose=2)\n",
    "model_loss_test, model_accuracy_test = deep_model.evaluate(\n",
    "        X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "print(model_accuracy_train)\n",
    "print(model_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model with 100% of data for unknown determinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)\n",
    "\n",
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "y_array = y.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_array)\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "\n",
    "# turn panda dataseries into numpy arrays\n",
    "X_array = X.values.copy()\n",
    "\n",
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "X_array_finite = imp.transform(X_array)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_array_finite)\n",
    "X_array_scaled = scaler.transform(X_array_finite)\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "y_categorical = to_categorical(encode_y)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "number_hidden_nodes = 18\n",
    "number_inputs = 25\n",
    "number_classes = 4\n",
    "\n",
    "deep_model = Sequential()\n",
    "\n",
    "deep_model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=number_inputs))\n",
    "\n",
    "for a in range(7):\n",
    "    deep_model.add(Dense(units=number_hidden_nodes, activation='relu'))\n",
    "\n",
    "deep_model.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "deep_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "        X_array_scaled,\n",
    "        y_categorical,\n",
    "        epochs=1000,\n",
    "        shuffle=True,\n",
    "        verbose=2,\n",
    "        workers=3\n",
    "        )\n",
    "\n",
    "model_loss_train, model_accuracy_train = deep_model.evaluate(\n",
    "        X_array_scaled, y_categorical, verbose=2)\n",
    "\n",
    "print(model_accuracy_train)\n",
    "\n",
    "deep_path_website = os.path.join(home, \"models\", \"website\", 'deep_neural_7_hidden_1000_epoch.h5')\n",
    "\n",
    "deep_model.save(deep_path_website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try gradient boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "y_array = y.values.copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "X_array = X.values.copy()\n",
    "\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_array)\n",
    "\n",
    "encode_y = label_encoder.transform(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, encode_y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=encode_y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train)\n",
    "X_train_finite = imp.transform(X_train)\n",
    "X_test_finite = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train_finite)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_finite)\n",
    "X_test_scaled = scaler.transform(X_test_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "GB_path = os.path.join(home, \"models\", 'Gradient_Boosted.joblib')\n",
    "\n",
    "dump(gbm, GB_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model with 100% of data for unknown determinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)\n",
    "\n",
    "# Stop all the futurewarnings\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "y_array = y.values.copy()\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "X_array = X.values.copy()\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_array)\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "# check the shape of the arrays\n",
    "print(encode_y.shape)\n",
    "print(X_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_array)\n",
    "X_array_finite = imp.transform(X_array)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_array_finite)\n",
    "X_array_scaled = scaler.transform(X_array_finite)\n",
    "\n",
    "\n",
    "# Algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(n_estimators=400, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "gbm.fit(X_array_scaled, encode_y)\n",
    "\n",
    "print(gbm.score(X_array_scaled, encode_y))\n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "gbm_website = os.path.join(home, \"models\", \"website\", 'Gradient_Boosted.joblib')\n",
    "dump(gbm, gbm_website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think 4 models are enough\n",
    "# These 4 models will be called for predictions as to which great lake the sample came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
