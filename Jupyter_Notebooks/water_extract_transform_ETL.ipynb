{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 3 seperate datasets that will be queried\n",
    "# First up is \"Water chemistry (Great Lakes nearshore areas)\"\n",
    "[Link](https://www.ontario.ca/data/water-chemistry-great-lakes-nearshore-areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "raw_data = os.path.join(home, \"raw_data\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(raw_data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway to file\n",
    "excel_path = os.path.join(raw_data, \"Great_Lakes_Chemistry\", \"GLIS_2000_2015_WATER_CHEMISTRY.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sheet names\n",
    "import xlrd\n",
    "\n",
    "xls = xlrd.open_workbook(filename=excel_path, on_demand=True)\n",
    "sheet_names = xls.sheet_names()\n",
    "print(sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of lakes that are in the excel file\n",
    "lakes = [\"huron\", \"erie\", \"ontario\", \"superior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all dataframes to 1 dataframe using pd.concat()\n",
    "dataframe_list = []\n",
    "\n",
    "for x in range(len(sheet_names)):\n",
    "    \n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_names[x])\n",
    "    df.insert(loc=0, column='lake', value=lakes[x])\n",
    "    \n",
    "    dataframe_list.append(df)\n",
    "    del df\n",
    "    \n",
    "df_combined1 = pd.concat(dataframe_list)\n",
    "\n",
    "df_combined1.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get row number and column number\n",
    "df_combined1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns, drop unwanted columns and drop any nulls\n",
    "df_combined1.columns = [\n",
    "    'lake', 'water_body', 'date_collect', 'sample_num', 'matrix',\n",
    "    'sample_type', 'sample_descr', 'remove1',\n",
    "    'remove2', 'remove3', 'station_num', 'station_descr',\n",
    "    'latitude', 'longitude', 'water_depth', 'sample_depth',\n",
    "    'secchi_depth', 'lims_method', 'lims_product',\n",
    "    'analyte', 'test_code', 'result', 'unit',\n",
    "    'remove4', 'remove5'\n",
    "]\n",
    "\n",
    "drop_columns = [\n",
    "    'matrix', 'sample_type', 'sample_descr', 'remove1',\n",
    "    'remove2', 'remove3', 'remove4', 'remove5',\n",
    "    'water_depth', 'sample_depth', 'secchi_depth',\n",
    "    'lims_method', 'lims_product', 'test_code'\n",
    "]\n",
    "\n",
    "df_combined2 = df_combined1.copy()\n",
    "\n",
    "df_combined2.drop(drop_columns, axis=1, inplace=True)\n",
    "df_combined2.dropna(inplace=True)\n",
    "\n",
    "df_combined2[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of several types of analytes, not useful for machine learning\n",
    "# focus on inorganic and some organic ones only\n",
    "df_clean1 = df_combined2.copy()\n",
    "\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='Microcystin.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='.*microcystin.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='Perflu.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='.*chloro.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='.*fluor.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='Pyrethrin.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='.*Chlordane', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='Endosulphan.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='Heptachlor.*', value = '-', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='PCB.*', value = '-', regex = True)\n",
    "\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"-\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"result\"] != \"-\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != 'Conductivity Estimated']\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != 'Chlorophyll - a; corrected']\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != 'Nitrogen; nitrite']\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != 'Nitrogen; total Kjeldahl']\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Ion balance calculation\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Anions\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Cations\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Langeliers index calculation\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Anatoxin-A\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"pp-DDD\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"pp-DDE\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"pp-DDT\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Toxaphene\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Trifluralin\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Permethrin\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Piperonyl Butoxide\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Aldrin\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Dieldrin\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Methoxychlor\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Endrin\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Mirex\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Oxychlordane\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"op-DDT\"]\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"Saturation pH Estimated\"]\n",
    "\n",
    "df_clean1[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name of several values\n",
    "# will aggregate these values specifically\n",
    "df_clean2 = df_clean1.copy()\n",
    "\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Solids.*', value = 'solids', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='.*ammonia.*', value = 'ammonia', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='.*nitrate.*', value = 'nitrate_ite', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Nitrogen.*', value = 'nitrogen', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Alkalinity.*', value = 'alkaline', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Phosphorus.*', value = 'phosphorus', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='.* pH .*', value = 'pH', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Carbon.*', value = 'carbon', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Silic.*', value = 'silicon', regex = True)\n",
    "df_clean2['analyte'] = df_clean2[\"analyte\"].replace(to_replace ='Chlorophyll.*', value = 'chlorophyll', regex = True)\n",
    "\n",
    "\n",
    "df_clean2[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all values in \"analyte\" column\n",
    "# will make these values into columns later\n",
    "df_clean3 = df_clean2.copy()\n",
    "df_clean3['analyte'] = df_clean3['analyte'].str.lower()\n",
    "df_clean3[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all value in \"result\" column is float\n",
    "# no need to clean up any strings\n",
    "np.array_equal(df_clean3.result, df_clean3.result.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after all of these transformations view the shape of the dataframe\n",
    "df_clean3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean4 = df_clean3.copy()\n",
    "\n",
    "# change so all units are the same\n",
    "# convert the units as well\n",
    "# ph as unitless numbers and turbidity is measured by FTU\n",
    "# make a note of this\n",
    "df_clean4.loc[df_clean4.unit == \"MG/L\", 'result'] *= 1000000\n",
    "df_clean4.loc[df_clean4.unit == \"UG/L\", 'result'] *= 1000\n",
    "df_clean4.loc[df_clean4.unit == \"MG/L\", 'unit'] = \"ng/L\"\n",
    "df_clean4.loc[df_clean4.unit == \"UG/L\", 'unit'] = \"ng/L\"\n",
    "df_clean4.loc[df_clean4.unit == \"NG/L\", 'unit'] = \"ng/L\"\n",
    "\n",
    "df_clean4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pivot the dataframe so that analytes become the column headers\n",
    "# fix the columns from multiindex\n",
    "df_pivot = df_clean4.copy()\n",
    "df_pivot = pd.pivot_table(df_pivot, values=['result'],\\\n",
    "                          index=['lake', 'water_body', \"date_collect\", \"sample_num\",\\\n",
    "                                 \"station_num\", \"station_descr\", \"latitude\",\\\n",
    "                                 \"longitude\"], columns=['analyte'], aggfunc=np.sum)\n",
    "\n",
    "df_pivot.reset_index(level=['lake', 'water_body', 'date_collect',\\\n",
    "                            'sample_num', 'station_num', 'station_descr',\\\n",
    "                           'latitude', 'longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new shape is received for the data from the pivot\n",
    "# lose rows but gain columns\n",
    "df_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix the column headers\n",
    "df_pivot.columns = df_pivot.columns.droplevel(0)\n",
    "df_pivot.columns = ['lake', 'water_body', 'date_collect', 'sample_num', 'station_num',\n",
    "       'station_descr', 'latitude', 'longitude', 'alkaline', 'aluminum', 'ammonia',\n",
    "       'antimony', 'arsenic', 'barium', 'beryllium', 'boron', 'cadmium',\n",
    "       'calcium', 'carbon', 'chloride', 'chlorophyll', 'chromium', 'cobalt',\n",
    "       'conductivity', 'copper', 'hardness', 'iron', 'lead', 'magnesium',\n",
    "       'manganese', 'mercury', 'molybdenum', 'nickel', 'nitrate_ite',\n",
    "       'nitrogen', 'ph', 'phosphorus', 'potassium', 'selenium', 'silicon',\n",
    "       'silver', 'sodium', 'solids', 'strontium', 'sulphate', 'thallium',\n",
    "       'titanium', 'turbidity', 'uranium', 'vanadium', 'zinc'\n",
    "]\n",
    "\n",
    "df_pivot.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby by all descriptors in order to aggregate \"solids\", \"nitrogen\", \"phosphorous\", \"alkinility\",\n",
    "# silicon and carbon\n",
    "# the dataset split these values apart for better determination of element types\n",
    "df_groupby1 = df_pivot.copy()\n",
    "\n",
    "df_groupby1 = df_groupby1.groupby(['lake', 'water_body', 'date_collect', 'station_num',\\\n",
    "                                   'sample_num', 'station_descr', 'latitude', 'longitude']).sum()\n",
    "\n",
    "df_groupby1.reset_index(level=['lake', 'water_body', 'date_collect',\\\n",
    "                            'sample_num', 'station_num', 'station_descr',\\\n",
    "                           'latitude', 'longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turbidity and pH are special values\n",
    "# if pH = 0, that means the lake has battery acid for water, more then likely the aggregation did not perserve null values\n",
    "# also any negative numbers in any of the elemental concentrations do not make any sense\n",
    "# need to replace with more common sense numbers\n",
    "\n",
    "# list of columns\n",
    "less_than = [\n",
    "        'alkaline', 'aluminum',\n",
    "       'ammonia', 'antimony', 'arsenic', 'barium', 'beryllium', 'boron',\n",
    "       'cadmium', 'calcium', 'carbon', 'chloride', 'chlorophyll', 'chromium',\n",
    "       'cobalt', 'conductivity', 'copper', 'hardness', 'iron', 'lead',\n",
    "       'magnesium', 'manganese', 'mercury', 'molybdenum', 'nickel',\n",
    "       'nitrate_ite', 'nitrogen', 'ph', 'phosphorus', 'potassium', 'selenium',\n",
    "       'silicon', 'silver', 'sodium', 'solids', 'strontium', 'sulphate',\n",
    "       'thallium', 'titanium', 'turbidity', 'uranium', 'vanadium', 'zinc'\n",
    "]\n",
    "\n",
    "# replace anything less than 0 with 0\n",
    "for y in less_than:\n",
    "    df_groupby1.loc[df_groupby1[y] <= 0, y] = 0\n",
    "    \n",
    "df_groupby1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all 0 with NaN\n",
    "df_groupby1.replace(0, np.nan, inplace=True)\n",
    "df_groupby1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby1.isna().sum()/df_groupby1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep these columns where the % of NaN is less than 40%, arbitrary decision\n",
    "from_first = ['aluminum', 'ammonia', 'barium', 'calcium', 'carbon',\n",
    "             'chloride', 'chlorophyll', 'chromium', 'conductivity',\n",
    "             'copper', 'hardness', 'magnesium', 'manganese', 'mercury',\n",
    "              'molybdenum', 'nitrate_ite', 'phosphorus', 'potassium',\n",
    "             'silicon', 'sodium', 'strontium', 'sulphate', 'turbidity',\n",
    "             'vanadium', 'zinc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "df_groupby1 = df_groupby1[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "       'station_descr', 'latitude', 'longitude'] + from_first]\n",
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change format of \"date_collect\" to YYYY-MM-DD to be in line with second dataset\n",
    "df_groupby1[\"date_collect\"] = df_groupby1[\"date_collect\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df_groupby1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows if the following rows are not populated\n",
    "df_groupby1.dropna(subset=from_first, thresh=1, inplace=True)\n",
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "first_dataset = df_groupby1[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "       'station_descr', 'latitude', 'longitude', 'conductivity', 'hardness', 'turbidity',\n",
    "        'chlorophyll', 'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "        'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury', 'molybdenum',\n",
    "        'phosphorus', 'potassium', 'silicon', 'sodium', 'strontium', 'sulphate', 'vanadium', 'zinc']].copy()\n",
    "\n",
    "first_metadata_dataset = df_groupby1[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\\\n",
    "       'station_descr', 'latitude', 'longitude']].copy()\n",
    "\n",
    "# save dataframe to .csv file\n",
    "first_data = os.path.join(clean_data, \"first.csv\")\n",
    "first_metadata = os.path.join(clean_data, \"first_metadata.csv\")\n",
    "\n",
    "first_dataset.to_csv(first_data)\n",
    "first_metadata_dataset.to_csv(first_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second up is \"Lake water quality at drinking water intakes\"\n",
    "[Link](https://www.ontario.ca/data/lake-water-quality-drinking-water-intakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "raw_data = os.path.join(home, \"raw_data\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(raw_data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get excel file names\n",
    "path = os.path.join(raw_data, \"Water_Quality_Drinking_Intakes\")\n",
    "\n",
    "files = []\n",
    "\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.xlsx' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sheet names\n",
    "import xlrd\n",
    "\n",
    "sheet_name_dict = {}\n",
    "\n",
    "files_index = [1, 2, 3, 5]\n",
    "\n",
    "for x in range(len(files_index)):\n",
    "    \n",
    "    xls = xlrd.open_workbook(filename=files[files_index[x]], on_demand=True)\n",
    "    sheet_names = {x : xls.sheet_names()}\n",
    "    \n",
    "    print(sheet_names)\n",
    "    \n",
    "    sheet_name_dict.update(sheet_names)\n",
    "    \n",
    "    del xls, sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all dataframes to 1 dataframe using pd.concat()\n",
    "dataframe_list_books = []\n",
    "\n",
    "for z in range(len(files_index)):\n",
    "    \n",
    "    dataframe_list_sheets = []\n",
    "    \n",
    "    for x in range(len(sheet_name_dict[z])):\n",
    "\n",
    "        df = pd.read_excel(files[files_index[z]], sheet_name=sheet_name_dict[z][x])\n",
    "\n",
    "        dataframe_list_sheets.append(df)\n",
    "        del df\n",
    "\n",
    "    df_combined1 = pd.concat(dataframe_list_sheets)\n",
    "    \n",
    "    dataframe_list_books.append(df_combined1)\n",
    "\n",
    "df_combined2 = pd.concat(dataframe_list_books)\n",
    "del dataframe_list_books\n",
    "\n",
    "df_combined2.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of this dataframe and make a copy\n",
    "df_second = df_combined2.copy()\n",
    "df_second.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second[\"TEST_DESCRIPTION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second[\"UNIT\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns, drop unwanted columns and drop any nulls\n",
    "df_second.columns = [\n",
    "        'sample_num', 'result_num', 'lake', 'station_num', 'location',\n",
    "        'date_collect', 'remove1', 'analyte', 'result', 'unit',\n",
    "        'remove2', 'remove3'\n",
    "]\n",
    "\n",
    "drop_columns = [\n",
    "    'result_num', 'remove1', 'remove2', 'remove3'\n",
    "]\n",
    "\n",
    "df_second2 = df_second.copy()\n",
    "\n",
    "df_second2.drop(drop_columns, axis=1, inplace=True)\n",
    "df_second2.dropna(inplace=True)\n",
    "\n",
    "df_second2[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of several types of analytes, not useful for machine learning\n",
    "# focus on inorganic and some organic ones only\n",
    "df_clean1 = df_second2.copy()\n",
    "\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='CHLOROPHYLL.*', value = 'chlorophyll', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='AMMONIUM.*', value = 'ammonia', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='NITRITE.*', value = 'nitrate_ite', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='NITRATE.*', value = 'nitrate_ite', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='PHOSPH.*', value = 'phosphorus', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='CARBON.*', value = 'carbon', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='SILICAT.*', value = 'silicon', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='CONDUCTIVITY.*', value = 'conductivity', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='PH .*', value = 'ph', regex = True)\n",
    "df_clean1['analyte'] = df_clean1[\"analyte\"].replace(to_replace ='CHLORIDE.*', value = 'chloride', regex = True)\n",
    "\n",
    "df_clean1 = df_clean1[df_clean1[\"analyte\"] != \"NITROGEN,TOT,KJELDAHL/UNF.REA\"]\n",
    "\n",
    "df_clean1[\"analyte\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all value in \"result\" column is float\n",
    "# no need to clean up any strings\n",
    "np.array_equal(df_clean1.result, df_clean1.result.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean1[\"unit\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = df_clean1.copy()\n",
    "\n",
    "# change so all units are the same\n",
    "# convert the units as well\n",
    "# ph as unitless numbers and turbidity is measured by FTU\n",
    "# make a note of this\n",
    "df_clean2.loc[df_clean2.unit == \"mg/L\", 'result'] *= 1000000\n",
    "df_clean2.loc[df_clean2.unit == \"ug/L\", 'result'] *= 1000\n",
    "df_clean2.loc[df_clean2.unit == \"mg/L\", 'unit'] = \"ng/L\"\n",
    "df_clean2.loc[df_clean2.unit == \"ug/L\", 'unit'] = \"ng/L\"\n",
    "\n",
    "# Change \"lake\" column values\n",
    "df_clean2.loc[df_clean2.lake == \"Lake Erie\", 'lake'] = \"erie\"\n",
    "df_clean2.loc[df_clean2.lake == \"Lake Huron\", 'lake'] = \"huron\"\n",
    "df_clean2.loc[df_clean2.lake == \"Lake Ontario\", 'lake'] = \"ontario\"\n",
    "df_clean2.loc[df_clean2.lake == \"Lake Superior\", 'lake'] = \"superior\"\n",
    "\n",
    "df_clean2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the dataframe so that analytes become the column headers\n",
    "# fix the columns from multiindex\n",
    "df_pivot = df_clean2.copy()\n",
    "df_pivot = pd.pivot_table(df_pivot, values=['result'],\\\n",
    "                          index=['sample_num', 'lake',\\\n",
    "                                 'station_num', 'location', 'date_collect'],\\\n",
    "                          columns=['analyte'], aggfunc=np.sum)\n",
    "\n",
    "df_pivot.reset_index(level=['sample_num', 'lake',\\\n",
    "                                 'station_num', 'location', 'date_collect'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new shape is received for the data from the pivot\n",
    "# lose rows but gain columns\n",
    "df_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the column headers\n",
    "df_pivot.columns = df_pivot.columns.droplevel(0)\n",
    "df_pivot.columns = [\n",
    "    'sample_num', 'lake',\n",
    "    'station_num', 'location', 'date_collect',\n",
    "    'ammonia', 'carbon', 'chloride', 'chlorophyll',\n",
    "    'conductivity', 'nitrate_ite', 'ph', 'phosphorous', 'silicon'\n",
    "]\n",
    "\n",
    "df_pivot.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby by all descriptors in order to aggregate \"ammonia\", \"nitrates and nitrites\", \"phosphorus\",\n",
    "# silicon and carbon\n",
    "# the dataset split these values apart for better determination of element types\n",
    "df_groupby1 = df_pivot.copy()\n",
    "\n",
    "df_groupby1 = df_groupby1.groupby(['sample_num', 'lake',\\\n",
    "                                'station_num', 'location', 'date_collect']).sum()\n",
    "\n",
    "df_groupby1.reset_index(level=['sample_num', 'lake',\\\n",
    "                                'station_num', 'location', 'date_collect'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turbidity and pH are special values\n",
    "# if pH = 0, that means the lake has battery acid for water, more then likely the aggregation did not perserve null values\n",
    "# also any negative numbers in any of the elemental concentrations do not make any sense\n",
    "# need to replace with more common sense numbers\n",
    "\n",
    "# list of columns\n",
    "less_than = [\n",
    "        'ammonia', 'carbon', 'chloride', 'chlorophyll',\n",
    "       'conductivity', 'nitrate_ite', 'ph', 'phosphorous', 'silicon'\n",
    "]\n",
    "\n",
    "# replace anything less than 0 with 0\n",
    "for y in less_than:\n",
    "    df_groupby1.loc[df_groupby1[y] <= 0, y] = 0\n",
    "    \n",
    "df_groupby1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all 0 with NaN\n",
    "df_groupby1.replace(0, np.nan, inplace=True)\n",
    "df_groupby1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby1.isna().sum()/df_groupby1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep these columns where the % of NaN is less than 60%, arbitrary decision\n",
    "from_second = ['ammonia', 'carbon', 'chloride', 'chlorophyll',\n",
    "                'conductivity', 'nitrate_ite', 'phosphorous', 'silicon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "df_groupby1 = df_groupby1[['lake', 'date_collect', 'station_num', 'sample_num',\n",
    "       'location'] + from_second]\n",
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate lists with google determined latitudes and longitudes\n",
    "import requests\n",
    "import json\n",
    "from API_KEY.apikey import API_KEY\n",
    "\n",
    "list_of_stations = df_groupby1[\"location\"].unique()\n",
    "\n",
    "lats = []\n",
    "longs = []\n",
    "\n",
    "for a in list_of_stations:\n",
    "\n",
    "    url = f\"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input={a}&inputtype=textquery&fields=name,geometry&locationbias=point:lat,lng&key={API_KEY}\"\n",
    "\n",
    "    lat_lng = requests.get(url)\n",
    "    print(\"success\")\n",
    "    lat_lng = lat_lng.json()\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        lats.append(lat_lng[\"candidates\"][0][\"geometry\"][\"location\"][\"lat\"])\n",
    "        longs.append(lat_lng[\"candidates\"][0][\"geometry\"][\"location\"][\"lng\"])\n",
    "        del lat_lng\n",
    "        \n",
    "    except:\n",
    "        lats.append(None)\n",
    "        longs.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = {\n",
    "    \"location\": list_of_stations,\n",
    "    \"latitude\": lats,\n",
    "    \"longitude\": longs\n",
    "}\n",
    "df_location = pd.DataFrame.from_dict(dict_df)\n",
    "df_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df_location, df_groupby1, how='inner', on=\"location\").copy()\n",
    "merge_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns filled with NaN to match with the first dataset\n",
    "merge_df[\"water_body\"] = None\n",
    "merge_df[\"hardness\"] = None\n",
    "merge_df[\"turbidity\"] = None\n",
    "merge_df[\"aluminum\"] = None\n",
    "merge_df[\"barium\"] = None\n",
    "merge_df[\"calcium\"] = None\n",
    "merge_df[\"chromium\"] = None\n",
    "merge_df[\"copper\"] = None\n",
    "merge_df[\"magnesium\"] = None\n",
    "merge_df[\"manganese\"] = None\n",
    "merge_df[\"mercury\"] = None\n",
    "merge_df[\"molybdenum\"] = None\n",
    "merge_df[\"potassium\"] = None\n",
    "merge_df[\"sodium\"] = None\n",
    "merge_df[\"strontium\"] = None\n",
    "merge_df[\"sulphate\"] = None\n",
    "merge_df[\"vanadium\"] = None\n",
    "merge_df[\"zinc\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows if the following rows are not populated\n",
    "merge_df.dropna(subset=from_second, thresh=1, inplace=True)\n",
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "merge_df.rename(columns={\"location\": \"station_descr\", \"phosphorous\": \"phosphorus\"}, inplace=True)\n",
    "\n",
    "merge_df = merge_df[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "       'station_descr', 'latitude', 'longitude', 'conductivity', 'hardness', 'turbidity',\n",
    "        'chlorophyll', 'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "        'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury', 'molybdenum',\n",
    "        'phosphorus', 'potassium', 'silicon', 'sodium', 'strontium', 'sulphate', 'vanadium', 'zinc']]\n",
    "merge_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "second_dataset = merge_df[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "       'station_descr', 'latitude', 'longitude', 'conductivity', 'hardness', 'turbidity',\n",
    "        'chlorophyll', 'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "        'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury', 'molybdenum',\n",
    "        'phosphorus', 'potassium', 'silicon', 'sodium', 'strontium', 'sulphate', 'vanadium', 'zinc']].copy()\n",
    "\n",
    "second_metadata_dataset = merge_df[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "                       'station_descr', 'latitude', 'longitude']].copy()\n",
    "\n",
    "# save dataframe to .csv file\n",
    "second_data = os.path.join(clean_data, \"second.csv\")\n",
    "second_metadata = os.path.join(clean_data, \"second_metadata.csv\")\n",
    "\n",
    "second_dataset.to_csv(second_data)\n",
    "second_metadata_dataset.to_csv(second_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third up is \"Drinking Water Surveillance Program\"\n",
    "[Link](https://www.ontario.ca/data/drinking-water-surveillance-program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadly this dataset does not identify which great lake is the source of water for each municipal system\n",
    "# Let us now merge the above 2 datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "raw_data = os.path.join(home, \"raw_data\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(raw_data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_csv = os.path.join(clean_data, \"first.csv\")\n",
    "\n",
    "first_df = pd.read_csv(first_csv)\n",
    "first_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "first_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_csv = os.path.join(clean_data, \"second.csv\")\n",
    "\n",
    "second_df = pd.read_csv(second_csv)\n",
    "second_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "second_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the columns equal\n",
    "first_df.columns == second_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the 2 dataframes\n",
    "complete_df = pd.concat([first_df, second_df])\n",
    "complete_df.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by \"lake\" and \"sample_num\"\n",
    "complete_df.sort_values(by=['lake', 'sample_num'], ascending=True, na_position='first', inplace=True)\n",
    "complete_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes\n",
    "total_dataset = complete_df[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "       'station_descr', 'latitude', 'longitude', 'conductivity', 'hardness', 'turbidity',\n",
    "        'chlorophyll', 'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "        'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury', 'molybdenum',\n",
    "        'phosphorus', 'potassium', 'silicon', 'sodium', 'strontium', 'sulphate', 'vanadium', 'zinc']].copy()\n",
    "\n",
    "total_metadata_dataset = complete_df[['lake', 'water_body', 'date_collect', 'station_num', 'sample_num',\n",
    "                       'station_descr', 'latitude', 'longitude']].copy()\n",
    "\n",
    "total_ML_dataset = complete_df[['lake', 'conductivity', 'hardness', 'turbidity',\n",
    "        'chlorophyll', 'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "        'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury', 'molybdenum',\n",
    "        'phosphorus', 'potassium', 'silicon', 'sodium', 'strontium', 'sulphate', 'vanadium', 'zinc']].copy()\n",
    "\n",
    "# save dataframe to .csv file\n",
    "total_data = os.path.join(clean_data, \"total.csv\")\n",
    "total_metadata = os.path.join(clean_data, \"total_metadata.csv\")\n",
    "total_ML_data = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "total_dataset.to_csv(total_data, index=False)\n",
    "total_metadata_dataset.to_csv(total_metadata, index=False)\n",
    "total_ML_dataset.to_csv(total_ML_data, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
