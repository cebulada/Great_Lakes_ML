{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us create several databases in postgreSQL that will be used to create FLASK APIs for the endpoint to call upon for visualization and for model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows and # of columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in .csv file, all 43 of them\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df[['conductivity', 'hardness', 'turbidity', 'chlorophyll',\n",
    "       'ammonia', 'nitrate_ite', 'aluminum', 'barium', 'calcium', 'carbon',\n",
    "       'chloride', 'chromium', 'copper', 'magnesium', 'manganese', 'mercury',\n",
    "       'molybdenum', 'phosphorus', 'potassium', 'silicon', 'sodium',\n",
    "       'strontium', 'sulphate', 'vanadium', 'zinc']].mean().copy()\n",
    "df_mean\n",
    "pd_series = pd.Series([\"Total\"], index=[\"lake\"])\n",
    "total = pd_series.append(df_mean)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = df.groupby(\"lake\").mean().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby.reset_index(inplace=True)\n",
    "df_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total = df_groupby.append(total, ignore_index=True)\n",
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = df[\"lake\"].unique().copy()\n",
    "\n",
    "label_encoder.fit(y)\n",
    "encode_y = label_encoder.transform(y)\n",
    "\n",
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df = {\n",
    "    \"lake\": y,\n",
    "    \"lake_encode\": encode_y\n",
    "}\n",
    "df2 = pd.DataFrame(dic_df)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to AWS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from config.postgres import username, password\n",
    "database_name = \"Great_Lake_ML\"\n",
    "\n",
    "\n",
    "rds_connection_string = f\"postgresql://{username}:{password}@localhost:5432/{database_name}\"\n",
    "\n",
    "engine = create_engine(rds_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from encoded_lakes', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "df2.to_sql(name='encoded_lakes', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from lake_means', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "df_total.to_sql(name='lake_means', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up \"total.csv\" and export it to the postgresql database\n",
    "# read in .csv as pandas dataframe\n",
    "path_all = os.path.join(clean_data, \"total.csv\")\n",
    "\n",
    "df_all = pd.read_csv(path_all)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['date_collect'] =  pd.to_datetime(df_all['date_collect'], infer_datetime_format=True)\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from master_data', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "df_all.to_sql(name='master_data', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up \"total.csv\" and export it to the postgresql database\n",
    "# read in .csv as pandas dataframe\n",
    "path_meta = os.path.join(clean_data, \"total_metadata.csv\")\n",
    "\n",
    "df_meta = pd.read_csv(path_meta)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_meta['date_collect'] =  pd.to_datetime(df_meta['date_collect'], infer_datetime_format=True)\n",
    "df_meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from metadata', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "df_meta.to_sql(name='metadata', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up \"total.csv\" and export it to the postgresql database\n",
    "# read in .csv as pandas dataframe\n",
    "path_data = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df_data = pd.read_csv(path_data)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from data', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "df_data.to_sql(name='data', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put training and testing data into AWS postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# globally set max columns and max rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "home = os.path.abspath(\"\")\n",
    "clean_data = os.path.join(home, \"clean_data\")\n",
    "print(home)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in .csv as pandas dataframe\n",
    "path = os.path.join(clean_data, \"total_ML_data.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an X (data) and y (labels)\n",
    "y = df[\"lake\"].copy()\n",
    "y_array = y.values\n",
    "\n",
    "# use label encoder to replace string with numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_array)\n",
    "\n",
    "encode_y = label_encoder.transform(y_array)\n",
    "\n",
    "encode_y\n",
    "\n",
    "X = df.copy()\n",
    "X.drop([\"lake\"], axis=1, inplace=True)\n",
    "X_array = X.values\n",
    "\n",
    "encode_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state=42 for all models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_temp, X_test_temp, y_train, y_test = train_test_split(X_array, encode_y,\n",
    "                                                train_size=0.75,\n",
    "                                                test_size=0.25,\n",
    "                                                stratify=encode_y,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train_temp)\n",
    "X_train = imp.transform(X_train_temp)\n",
    "X_test = imp.transform(X_test_temp)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'conductivity': X_train[:, 0], 'hardness': X_train[:, 1],\\\n",
    "                       'turbidity': X_train[:, 2], 'chlorophyll': X_train[:, 3],\\\n",
    "                       'ammonia': X_train[:, 4], 'nitrate_ite': X_train[:, 5],\\\n",
    "                       'aluminum': X_train[:, 6], 'barium': X_train[:, 7],\\\n",
    "                       'calcium': X_train[:, 8], 'carbon': X_train[:, 9],\\\n",
    "                       'chloride': X_train[:, 10], 'chromium': X_train[:, 11],\\\n",
    "                       'copper': X_train[:, 12], 'magnesium': X_train[:, 13],\\\n",
    "                       'manganese': X_train[:, 14], 'mercury': X_train[:, 15],\\\n",
    "                       'molybdenum': X_train[:, 16], 'phosphorus': X_train[:, 17],\\\n",
    "                       'potassium': X_train[:, 18], 'silicon': X_train[:, 19],\\\n",
    "                       'sodium': X_train[:, 20], 'strontium': X_train[:, 21],\\\n",
    "                       'sulphate': X_train[:, 22], 'vanadium': X_train[:, 23],\\\n",
    "                       'zinc': X_train[:, 24]})\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({'conductivity': X_test[:, 0], 'hardness': X_test[:, 1],\\\n",
    "                       'turbidity': X_test[:, 2], 'chlorophyll': X_test[:, 3],\\\n",
    "                       'ammonia': X_test[:, 4], 'nitrate_ite': X_test[:, 5],\\\n",
    "                       'aluminum': X_test[:, 6], 'barium': X_test[:, 7],\\\n",
    "                       'calcium': X_test[:, 8], 'carbon': X_test[:, 9],\\\n",
    "                       'chloride': X_test[:, 10], 'chromium': X_test[:, 11],\\\n",
    "                       'copper': X_test[:, 12], 'magnesium': X_test[:, 13],\\\n",
    "                       'manganese': X_test[:, 14], 'mercury': X_test[:, 15],\\\n",
    "                       'molybdenum': X_test[:, 16], 'phosphorus': X_test[:, 17],\\\n",
    "                       'potassium': X_test[:, 18], 'silicon': X_test[:, 19],\\\n",
    "                       'sodium': X_test[:, 20], 'strontium': X_test[:, 21],\\\n",
    "                       'sulphate': X_test[:, 22], 'vanadium': X_test[:, 23],\\\n",
    "                       'zinc': X_test[:, 24]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.insert(0, \"lake\", y_train.tolist())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.insert(0, \"lake\", y_test.tolist())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from config.postgres import username, password\n",
    "database_name = \"Great_Lake_ML\"\n",
    "\n",
    "\n",
    "rds_connection_string = f\"postgresql://{username}:{password}@localhost:5432/{database_name}\"\n",
    "\n",
    "engine = create_engine(rds_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table names\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from test_lakes', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "test_df.to_sql(name='test_lakes', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table headers are set up correctly\n",
    "pd.read_sql_query('select * from train_lakes', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pandas dataframe to the postgreSQL database\n",
    "train_df.to_sql(name='train_lakes', con=engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
